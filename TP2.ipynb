{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202254ac-9948-4000-b20c-ea75c16a9cd1",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# <font size=\"6\" color=\"darkblue\">Trabalho Laboratorial 2</font>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## <font size=\"4\" color=\"black\">Instituto Superior de Engenharia de Lisboa</font>\n",
    "\n",
    "### Aprendizagem Automática\n",
    "\n",
    "#### Docente: G. Marques\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Realizado por:\n",
    "\n",
    "<p style=\"text-align: center;\"><font size=\"3\">Belarmino Rafael Sacate nº: 52057</font></p>\n",
    "\n",
    "<p style=\"text-align: center;\"><font size=\"3\">Miguel Ferreira nº: 51878</font></p>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### Data: 12 de Dezembro de 2025\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe181c-4638-4e30-a517-3412a2bbebc2",
   "metadata": {},
   "source": [
    "---\n",
    "# Introducao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a3bb5-4365-4ef9-940c-a19aa3cb6b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af6264e4-e055-4a97-9df1-6882485336ad",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8294ae67-4c3c-4728-91c6-7d2820fe1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad22e1-96a0-4a4d-83c0-d696c367fa44",
   "metadata": {},
   "source": [
    "---\n",
    "# Leitura do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3c822e6f-7aee-4461-acf8-a3db2f4d8d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn =\"imdbFull.p\"\n",
    "D = pickle.load(open(fn , 'rb'))\n",
    "D.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4b21e5bc-039f-4207-8caa-5b13d51464c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = D.data\n",
    "y = D.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592fe18-0eed-4ba4-ba35-414e7b7b05ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Leitura do Docs a bruto(sem filtragem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb8691-4ef5-4889-b293-00a26def9174",
   "metadata": {},
   "source": [
    "Numa primeira fase, recorremos a funcao TfidfVectorizer() sem parametros de filtragem para podermos perceber com que tipo de texto estamos a tratar, para depois conseguirmos identificar que tipo de filtragens poderao ser implementadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b3ffdf13-dfae-464a-aa18-13330e42670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101895\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer().fit(Docs)\n",
    "tokens = tfidf.get_feature_names_out()\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a29e90-62c3-4edb-a655-59eb3f61db53",
   "metadata": {},
   "source": [
    "aqui podemos observar que temos cerca de 101895 tonkens, dos quais temos:\n",
    "\n",
    "\n",
    "- Elementos que como underscores; \n",
    "\n",
    "- caracteres numericos, entre outros;\n",
    "\n",
    "Estes serao retirados no processo de filtragem, que sera essencial para a limpeza do documento reduzindo o tamanho do mesmo e selecionando somente informacoes mais relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ab98c70c-e66f-43cc-b9b6-979dca8af4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6200', '62229249', '623', '624', '63', '637', '63rd', '64', '65',\n",
       "       '651', '65m', '65mm', '66', '660', '6600f', '666', '66er', '66p',\n",
       "       '66th', '67', '6723', '678', '67th', '68', '68th', '69', '69ing',\n",
       "       '69p', '69th', '6am', '6b', '6f', '6ft', '6hours', '6k', '6m',\n",
       "       '6million', '6pm', '6th', '6wks', '6yo', '6yrs', '70', '700',\n",
       "       '7000', '70000', '700k', '701', '70369862', '707', '70ies', '70km',\n",
       "       '70m', '70mm', '70p', '70s', '70th', '70õs', '71', '710', '713',\n",
       "       '72', '720', '727', '729', '72nd', '73', '7300', '735', '737',\n",
       "       '73min', '74', '740', '740il', '747', '747s', '74sooner', '74th',\n",
       "       '75', '750', '75054', '757', '75c', '75m', '75min', '76', '7600',\n",
       "       '762', '767', '77', '775', '78', '788', '78rpm', '79', '79th',\n",
       "       '7c', '7days', '7even', '7eventy', '7ft', '7ish', '7m', '7million',\n",
       "       '7mm', '7s', '7th', '7½', '7½th', '80', '800', '8000', '802',\n",
       "       '80ies', '80ish', '80min', '80s', '80yr', '81', '810', '814',\n",
       "       '817', '819', '82', '820', '8217', '8230', '83', '837', '83mins',\n",
       "       '84', '849', '84f', '84s', '84th', '85', '850', '850pm', '851',\n",
       "       '85min', '85mins', '86', '863', '86s', '87', '8700', '8763', '878',\n",
       "       '87mins', '87minutes', '88', '881', '886', '8888', '88min', '89',\n",
       "       '89or', '89s', '8bit', '8ftdf', '8k', '8mm', '8o', '8p', '8pm',\n",
       "       '8star', '8th', '8u', '8x', '8yrs', '8½', '90', '900', '9000',\n",
       "       '900000', '90210', '905', '90c', '90ies', '90ish', '90min',\n",
       "       '90mins', '90minutes', '90s', '91', '911', '914', '917', '92',\n",
       "       '921', '924', '92fs', '92nd', '93', '937', '93mins', '94', '9484',\n",
       "       '94s', '94th', '95', '950', '95th', '96', '96th', '97', '970',\n",
       "       '974th', '976', '978', '98', '985', '987', '98mins', '98minutes',\n",
       "       '99', '998', '999', '9999', '99999999999999999', '99cent',\n",
       "       '99cents', '99p', '99½', '9_', '9am', '9as', '9do', '9ers', '9is',\n",
       "       '9lbs', '9mm', '9of10', '9pm', '9s', '9th', '9ya', '__', '___',\n",
       "       '____', '_____', '______', '_______', '________', '_________',\n",
       "       '_____________________________',\n",
       "       '____________________________________',\n",
       "       '_____________________________________',\n",
       "       '______________________________________',\n",
       "       '______________________________________________',\n",
       "       '________________________________________________________________',\n",
       "       '__________________________________________________________________',\n",
       "       '___is', '_a', '_about_', '_absolute', '_absurdism', '_all_',\n",
       "       '_almost_', '_am_', '_amadeus_', '_and_', '_angel_', '_annie_',\n",
       "       '_any_', '_anyone_', '_anything_', '_apocalyptically', '_are_',\n",
       "       '_arlington', '_as', '_atlantis', '_atlantis_', '_attack',\n",
       "       '_before_', '_blair', '_boring_', '_both_', '_bounce_', '_brice',\n",
       "       '_brooklyn_', '_by', '_can', '_cannon_', '_cave', '_certainly_',\n",
       "       '_comedy_', '_compadres', '_could', '_cruel', '_dead_', '_dirty',\n",
       "       '_discuss_', '_discussing_', '_divinatory', '_do_', '_dr',\n",
       "       '_dying', '_earned_', '_ever_'], dtype=object)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1200:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25085b95-5ca3-4424-8c72-b954f4eb098c",
   "metadata": {},
   "source": [
    "Exemplo de doc antes da limpeza, elementos que como tags html que sao desnecessarios no nosso caso pois os estao a ocupar espaco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "be4b03e4-953a-4c70-b545-b9012be02a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Docs[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9a852-c999-4183-b7f0-1950c8a19b11",
   "metadata": {},
   "source": [
    "---\n",
    "# Limpeza dos Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b522641-4a19-4389-ae1b-0e22512fa622",
   "metadata": {},
   "source": [
    "Para a limpeza dos dados foi efectuada em 2 etapas:\n",
    "\n",
    "- Primeira Etapa\n",
    "\n",
    "    Substituicao de tags html por espacos vazios e\n",
    "\n",
    "\n",
    "    na selecao de somente caracteres afabeticos, e acentos nas linguas latinas\n",
    "\n",
    "\n",
    "- Segunda Etapa\n",
    "\n",
    "    uso da funcao TfidfVectorizer para selecionar palavras que ocorrem no minimo 5 vezes entre os Docs e que tenham 4 ou mais cararacteres, eliminando assim casos como por exemplo: \"the, a\" etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "346f7bc8-b6ec-4384-b186-e5a7f6c5a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = [doc.replace('<br />', ' ') for doc in Docs]\n",
    "Docs = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e307f849-b9ae-4252-9280-c6199d8c63e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35205\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, token_pattern=r'\\b\\w\\w\\w\\w+\\b').fit(Docs)\n",
    "tokens = tfidf.get_feature_names_out()\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82acde-1c37-4e98-a3b9-ba2c4722cc3b",
   "metadata": {},
   "source": [
    "Apos, a limpeza dos docs podemos notar uma dimuicao consideravel saimos de 101895 tokens para \n",
    "35205"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3902f9-5280-4432-91a8-35f91a1a86df",
   "metadata": {},
   "source": [
    "display dos 100 primeiros tokens apos a limpeza e filtragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c25c83df-ad4a-43b4-ad2d-b70c318d7b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaargh', 'aaliyah', 'aames', 'aamir', 'aankhen', 'aapke',\n",
       "       'aardman', 'aaron', 'aback', 'abandon', 'abandoned', 'abandoning',\n",
       "       'abandonment', 'abandons', 'abba', 'abbas', 'abbey', 'abbie',\n",
       "       'abbot', 'abbott', 'abbreviated', 'abby', 'abdomen', 'abduct',\n",
       "       'abducted', 'abducting', 'abduction', 'abductions', 'abductor',\n",
       "       'abducts', 'abdul', 'abel', 'aberration', 'aberrations', 'abetted',\n",
       "       'abhay', 'abhishek', 'abhorrent', 'abhors', 'abide', 'abiding',\n",
       "       'abigail', 'abilities', 'ability', 'abit', 'abject', 'ablaze',\n",
       "       'able', 'ably', 'abner', 'abnormal', 'abnormally', 'aboard',\n",
       "       'abode', 'abolished', 'abominable', 'abominably', 'abomination',\n",
       "       'abominations', 'aboriginal', 'aboriginals', 'aborigine',\n",
       "       'aborigines', 'abort', 'aborted', 'abortion', 'abortions',\n",
       "       'abortive', 'abound', 'abounds', 'about', 'above', 'abraham',\n",
       "       'abrahams', 'abrams', 'abrasive', 'abre', 'abridged', 'abril',\n",
       "       'abroad', 'abrupt', 'abruptly', 'absence', 'absences', 'absent',\n",
       "       'absentee', 'absolute', 'absolutely', 'absolution', 'absolutly',\n",
       "       'absolve', 'absorb', 'absorbed', 'absorbing', 'absorbs',\n",
       "       'absorption', 'abstinence', 'abstract', 'abstracted',\n",
       "       'abstraction'], dtype=object)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8999d0-3194-45e1-8c3d-037b1aab83dc",
   "metadata": {},
   "source": [
    "display do mesmo docs previamente ilustrado, mas desta vez limpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4c0c474f-1276-4a55-a83e-5a329603e20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zero Day leads you to think even re think why two boys young men would do what they did commit mutual suicide via slaughtering their classmates It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction It is not a perfect movie but given what money time the filmmaker and actors had it is a remarkable product In terms of explaining the motives and actions of the two young suicide murderers it is better than Elephant in terms of being a film that gets under our rationalistic skin it is a far far better film than almost anything you are likely to see Flawed but honest with a terrible honesty '"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Docs[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76793b61-7236-4e45-87ec-432058d31ebb",
   "metadata": {},
   "source": [
    "Apos a limpeza dos dados, foi feita a reducao de tokens com certa similaridade a um so token recorrendo ao PorterStemmer, consigos reduzir ainda mais os tokens saindo de 35205 para 30954"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fbda3e-77a6-43c0-bc45-37a455e8ecea",
   "metadata": {},
   "source": [
    "---\n",
    "# Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cbcb6362-c33f-489c-9a64-76b1e3bd0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemFunc = PorterStemmer()\n",
    "voc = tfidf.get_feature_names_out()\n",
    "voc2 = [stemFunc.stem(w) for w in voc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "67f159d6-7322-4e52-81f7-98b2576bb722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30954\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemFunc = PorterStemmer()\n",
    "Docs2 = [' '.join([stemFunc.stem(w) for w in d.split()]) for d in Docs]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(Docs2)\n",
    "tokens = tfidf.get_feature_names_out()\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "62e23fce-af43-4248-9793-f30fb1bfa2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa' 'aaah' 'aaargh' 'aag' 'aah' 'aaliyah' 'aam' 'aamir' 'aankhen'\n",
      " 'aapk' 'aardman' 'aargh' 'aaron' 'aatish' 'aback' 'abandon' 'abba'\n",
      " 'abbey' 'abbi' 'abbot' 'abbott' 'abbrevi' 'abc' 'abdalla' 'abdic'\n",
      " 'abdomen' 'abduct' 'abductor' 'abdul' 'abdullah' 'abe' 'abel'\n",
      " 'abercrombi' 'aberdeen' 'abernathi' 'abernethi' 'aberr' 'abet' 'abhay'\n",
      " 'abhi' 'abhishek' 'abhor' 'abhorr' 'abi' 'abid' 'abigail' 'abil' 'abit'\n",
      " 'abject' 'abl' 'ablaz' 'abli' 'abner' 'abnorm' 'abo' 'aboard' 'abod'\n",
      " 'abolish' 'abolit' 'abolitionist' 'abomin' 'aborigin' 'abort'\n",
      " 'abortionist' 'abound' 'about' 'abov' 'abr' 'abraham' 'abram' 'abras'\n",
      " 'abreast' 'abridg' 'abril' 'abroad' 'abrupt' 'abruptli' 'abscond'\n",
      " 'absenc' 'absent' 'absente' 'absentia' 'absolut' 'absolutley' 'absolutli'\n",
      " 'absolv' 'absorb' 'absorpt' 'abstain' 'abstin' 'abstract' 'absurd'\n",
      " 'absurdist' 'absurdli' 'abu' 'abund' 'abundantli' 'abus' 'abut' 'abuzz'\n",
      " 'abysm' 'abyss' 'acacia' 'acadami' 'academ' 'academi' 'academia'\n",
      " 'acapulco' 'acced' 'acceler' 'accent' 'accentu' 'accept' 'access'\n",
      " 'accessori' 'accid' 'accident' 'accidenti' 'acclaim' 'accolad' 'accommod'\n",
      " 'accomod' 'accompani' 'accompanist' 'accomplic' 'accomplish' 'accord'\n",
      " 'accordian' 'accordingli' 'accordion' 'accorsi' 'accost' 'account'\n",
      " 'accru' 'accumul' 'accur' 'accuraci' 'accus' 'accustom' 'ace' 'acerb'\n",
      " 'ach' 'acharya' 'achiev' 'achil' 'achingli' 'acid' 'ack' 'ackerman'\n",
      " 'ackland']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65644541-79ed-4f42-aa46-cd62b7280644",
   "metadata": {},
   "source": [
    "---\n",
    "# Representacao tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fbc5bf3a-f143-46dd-878f-d534a5349f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 30954) <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "x = tfidf.transform(Docs2)\n",
    "print(x.shape, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "039b27cc-a953-4818-899b-c7511a389983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa' 'aaah' 'aaargh' 'aag' 'aah' 'aaliyah' 'aam' 'aamir' 'aankhen'\n",
      " 'aapk' 'aardman' 'aargh' 'aaron' 'aatish' 'aback' 'abandon' 'abba'\n",
      " 'abbey' 'abbi' 'abbot' 'abbott' 'abbrevi' 'abc' 'abdalla' 'abdic'\n",
      " 'abdomen' 'abduct' 'abductor' 'abdul' 'abdullah' 'abe' 'abel'\n",
      " 'abercrombi' 'aberdeen' 'abernathi' 'abernethi' 'aberr' 'abet' 'abhay'\n",
      " 'abhi' 'abhishek' 'abhor' 'abhorr' 'abi' 'abid' 'abigail' 'abil' 'abit'\n",
      " 'abject' 'abl' 'ablaz' 'abli' 'abner' 'abnorm' 'abo' 'aboard' 'abod'\n",
      " 'abolish' 'abolit' 'abolitionist' 'abomin' 'aborigin' 'abort'\n",
      " 'abortionist' 'abound' 'about' 'abov' 'abr' 'abraham' 'abram' 'abras'\n",
      " 'abreast' 'abridg' 'abril' 'abroad' 'abrupt' 'abruptli' 'abscond'\n",
      " 'absenc' 'absent' 'absente' 'absentia' 'absolut' 'absolutley' 'absolutli'\n",
      " 'absolv' 'absorb' 'absorpt' 'abstain' 'abstin' 'abstract' 'absurd'\n",
      " 'absurdist' 'absurdli' 'abu' 'abund' 'abundantli' 'abus' 'abut' 'abuzz']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc216f-f131-4d3c-949b-bbf68c40f035",
   "metadata": {},
   "source": [
    "Organizacao das palavras em ordem crescente, tendo em conta a ordenacao do tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ab317d8d-245b-4f2e-bf56-8a72a22c13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "xM = np.max(x,axis=0).toarray().squeeze()\n",
    "idx = np.argsort(-xM)\n",
    "voc = [tokens[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b835b24-d838-4667-96d9-975214b4a1c4",
   "metadata": {},
   "source": [
    "print das palavras que o modelo tfidx considera mais importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9c3be006-1a56-455f-b877-ee1f07a39023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pokemon', 'ghouli', 'dalmat', 'scanner', 'robot', 'uzumaki', 'doodlebop', 'bad', 'esperanto', 'cycl', 'critter', 'rodrigu', 'ernest', 'smallvil', 'nuke', 'sasquatch', 'wei', 'steve', 'lupin', 'tarzan', 'darkman', 'primari', 'zatoichi', 'woo', 'bye', 'colombo', 'shark', 'prot', 'kibbutz', 'farscap', 'wirey', 'shemp', 'fujimori', 'muppet', 'barney', 'akasha', 'gamera', 'columbo', 'joke', 'lennon', 'botch', 'woodburi', 'knott', 'brendan', 'weller', 'brynner', 'custer', 'rajni', 'hackenstein', 'wine', 'melt', 'naschi', 'ninja', 'gadget', 'dahmer', 'gruner', 'matrix', 'twelv', 'beller', 'othello', 'cypher', 'dariu', 'stepford', 'vestron', 'blah', 'duvivi', 'twister', 'hanzo', 'srk', 'pasteur', 'django', 'zizek', 'dev', 'marti', 'dean', 'fabian', 'beetl', 'moto', 'tibb', 'gein', 'speck', 'hallam', 'winfield', 'janean', 'marathon', 'biko', 'and', 'noriko', 'suck', 'skate', 'seagal', 'genova', 'worm', 'alvin', 'game', 'camp', 'killjoy', 'kronk', 'sherpa', 'vick']\n"
     ]
    }
   ],
   "source": [
    "print(voc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858dd00-22df-4469-acb1-80700ca825be",
   "metadata": {},
   "source": [
    "print das palavras que o modelo tfidx considera menos importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "887bf7ab-2de8-44fd-8425-fb4aba5101b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gayatri', 'hiroshi', 'candidli', 'gazett', 'encamp', 'acced', 'beslon', 'suwa', 'vibrantli', 'takemitsu', 'citadel', 'lopsid', 'pintauro', 'domicil', 'pipsqueak', 'chessboard', 'motorcar', 'illusori', 'harrowingli', 'shukla', 'imposit', 'roadway', 'dykstra', 'greenland', 'wada', 'nagato', 'koizumi', 'asahina', 'traips', 'rancho', 'entreat', 'hesitantli', 'chokeslam', 'psychosomat', 'authori', 'rifleman', 'starkest', 'rateyourmus', 'awoken', 'bhatti', 'tokoro', 'megaphon', 'steamship', 'tradesmen', 'vala', 'funakoshi', 'fedor', 'shrubland', 'nassau', 'debauch', 'rueful', 'susten', 'popularis', 'accomod', 'mccowen', 'ardor', 'straightforwardli', 'zekeria', 'salesperson', 'cavalryman', 'sohrab', 'seydou', 'rowboat', 'purvey', 'perrier', 'unpleasantri', 'mahmoodzada', 'homayoun', 'ershadi', 'underpaid', 'prettifi', 'perfidi', 'undercard', 'gurind', 'choisi', 'turnout', 'zantara', 'readout', 'yammer', 'cartograph', 'bandekar', 'seminarian', 'pasternak', 'invocu', 'ddt', 'cavendish', 'arrowsmith', 'elham', 'ehsa', 'primu', 'atossa', 'kingsford', 'deific', 'fulgencio', 'yonder', 'opper', 'counterattack', 'estella', 'superplex', 'howland']\n"
     ]
    }
   ],
   "source": [
    "print(voc[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0938386-4759-497a-a067-c50920862df8",
   "metadata": {},
   "source": [
    "Como podemos notar existem palavras que foram muito reduzidas ao utilizarmos o PorterStemmer, e abaixo esta uma implmentacao da analise feita acima com as palavras norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ad02af8a-308d-4d40-867e-89006fb73882",
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = D.data\n",
    "Docs = [doc.replace('<br />', ' ') for doc in Docs]\n",
    "Docs = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs]\n",
    "tfidf = TfidfVectorizer(min_df=5, token_pattern=r'\\b\\w\\w\\w\\w+\\b').fit(Docs)\n",
    "tokens = tfidf.get_feature_names_out()\n",
    "x = tfidf.transform(Docs)\n",
    "xM = np.max(x, axis=0).toarray().squeeze()\n",
    "idx = np.argsort(-xM)\n",
    "voc = [tokens[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c55d7-6979-4544-a4bc-c4682a00fc55",
   "metadata": {},
   "source": [
    "print das palavras que o modelo tfidx considera mais importantes (sem PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "63df0888-f214-46c2-a0bb-6800c63800a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pokemon', 'scanners', 'robot', 'ghoulies', 'cycle', 'lupin', 'ernest', 'doodlebops', 'dalmations', 'critters', 'rodrigues', 'gamera', 'steve', 'tarzan', 'darkman', 'smallville', 'prot', 'fujimori', 'wine', 'sasquatch', 'jokes', 'demons', 'xica', 'casper', 'zatoichi', 'colombo', 'farscape', 'lennon', 'akasha', 'barney', 'cypher', 'wirey', 'sucks', 'gadget', 'custer', 'primary', 'weller', 'speck', 'shemp', 'naschy', 'brynner', 'janeane', 'worms', 'noriko', 'match', 'hackenstein', 'woodbury', 'ants', 'shark', 'botched', 'zizek', 'priya', 'dean', 'othello', 'stepford', 'game', 'marty', 'khouri', 'ninja', 'blah', 'gruner', 'brendan', 'duvivier', 'hanzo', 'matrix', 'elvira', 'paulie', 'alvin', 'joan', 'muppet', 'chiba', 'darius', 'killjoy', 'tanner', 'nemesis', 'nuke', 'hallam', 'gein', 'leonora', 'karloff', 'lexi', 'fabian', 'beatles', 'oprah', 'sissy', 'dentist', 'pack', 'cream', 'tyrannosaurus', 'zombi', 'coop', 'twelve', 'flea', 'buddy', 'blob', 'azumi', 'keaton', 'karen', 'alaska', 'mencia']\n"
     ]
    }
   ],
   "source": [
    "print(voc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d61d13-2b28-49bd-a282-cf6217c9f0c7",
   "metadata": {},
   "source": [
    "print das palavras que o modelo tfidx considera menos importantes (sem PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d97a77a2-14ae-4516-8382-703e7ff6440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cineplex', 'denigrating', 'emotes', 'understudy', 'credo', 'vertical', 'astride', 'backfire', 'chaplinesque', 'kinship', 'goads', 'yokels', 'radiating', 'puncture', 'unsupervised', 'compounds', 'proletariat', 'eatery', 'perpetrating', 'firecracker', 'organically', 'everyway', 'elaborating', 'slavering', 'immunity', 'plucking', 'boating', 'alphaville', 'tacks', 'whelan', 'cataclysmic', 'gingerly', 'scuppered', 'brims', 'dinky', 'unerring', 'père', 'subtracted', 'bafflingly', 'britt', 'striding', 'assassinates', 'bluntness', 'relinquishing', 'basking', 'sprang', 'rigging', 'allocated', 'alloy', 'nyree', 'entanglements', 'confections', 'nouveau', 'unmentioned', 'emits', 'ulliel', 'imposition', 'mclaughlin', 'unarguably', 'cardinals', 'schemer', 'imbuing', 'sharpshooter', 'luncheon', 'pervasively', 'auspicious', 'clarifying', 'institutionalised', 'sylvain', 'obstruction', 'expensively', 'indicted', 'devising', 'manoeuvre', 'vexed', 'galadriel', 'scolded', 'chokeslam', 'inscribed', 'ballyhara', 'landowners', 'ascends', 'harrowingly', 'steamship', 'illusory', 'fonts', 'uphold', 'décor', 'funakoshi', 'forges', 'fedor', 'rateyourmusic', 'antiquity', 'rueful', 'cavalryman', 'homayoun', 'ershadi', 'mahmoodzada', 'ridges', 'suplexes']\n"
     ]
    }
   ],
   "source": [
    "print(voc[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8581c-5ff6-49b9-a4b1-3dc1cae1d681",
   "metadata": {},
   "source": [
    "O modelo tdfidf considera essas palavras menos importantes pois elas aparecem em muitas e muitas criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce93d8-87d8-4cda-8eb4-ade9557d3bec",
   "metadata": {},
   "source": [
    "agora fazemos o contrario recorrendo ao inverse document frequency.\n",
    "\n",
    "\n",
    "quao mais baixo for o idf mais for o valor deste termo mais vezes a palavra aparece.\n",
    "\n",
    "As palavras abaixo sao as palavras que aparecem mais vezes independemente da critica ser boa ou ma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "dca4993c-9b4a-4f75-a0d6-f7c2f4b6007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'that', 'with', 'movie', 'have', 'film', 'from', 'like', 'they', 'there', 'just', 'about', 'what', 'some', 'good', 'when', 'more', 'time', 'very', 'even', 'only', 'would', 'really', 'well', 'which', 'story', 'much', 'than', 'their', 'were', 'other', 'been', 'most', 'also', 'into', 'first', 'great', 'will', 'made', 'because', 'people', 'make', 'could', 'after', 'them', 'then', 'watch', 'think', 'movies', 'acting', 'seen', 'characters', 'many', 'being', 'never', 'plot', 'know', 'ever', 'best', 'character', 'where', 'little', 'over', 'better', 'films', 'life', 'does', 'love', 'still', 'your', 'here', 'something', 'these', 'while', 'should', 'scenes', 'such', 'through', 'scene', 'back', 'watching', 'those', 'thing', 'real', 'actors', 'before', 'another', 'doesn', 'years', 'though', 'director', 'makes', 'didn', 'work', 'actually', 'look', 'find', 'nothing', 'going', 'show']\n"
     ]
    }
   ],
   "source": [
    "idf = tfidf.idf_ \n",
    "idx = np.argsort(idf)\n",
    "voc=[tokens[i] for i in idx]\n",
    "print(voc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b072c-3a9e-4050-a2cb-52c7f4bee51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23374373-6c2c-4118-a4bb-25246f15e498",
   "metadata": {},
   "source": [
    "---\n",
    "# Classificacao e Regressao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34a6df-209e-4706-9677-b5943cbd827d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca871d5-6d37-44a0-adc8-faf59276e716",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ea758-196e-4950-95b3-1229288a1fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad0fcdcb-68f6-4994-a36b-f5a102f08451",
   "metadata": {},
   "source": [
    "---\n",
    "# Introducao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08869b-f823-4fcb-b484-c1d5233db277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
